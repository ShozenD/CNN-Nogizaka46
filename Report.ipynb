{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the faces of Nogizaka 46 with CNN\n",
    "<p>Student ID: 71745242<br>\n",
    "Name: Shozen Dan <br>\n",
    "Class: Heuristic Computing <br>\n",
    "Instructor: Takefuji Yoshiyasu</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "<p>The other day, when I was surfing the internet for interesting machine learning ideas, I discovered a blog on Aidemy about facial identification[1]. The author is a fan of the popular idol group Nogizaka 46 and wanted to create a program that would identify the faces of his favorite 5 memebers. The steps he took are as simple. He obtained 100 images for each of the target members using Google's custom search API. Next he used the image processing library OpenCV to find the faces in each picture and crop them out. At this point there were only about 70 images remaining for each member, thus he augmented the data and increased the data size 8 times. Finally, he created a network using Keras and Tesorflow and achieved an accuracy of about 70 percent. I found another blog[2] trying to solve the same problem and the author achieved an accuracy of 75~80 percent. Although the problem its self is rather frivolous and have no practical applications, the steps involved in solving it can be applied widely. Thus the objective of my project is as follows</p>\n",
    "<ol>\n",
    "    <li>Learn the methods involved in dealing with small datasets for deep learning</li>\n",
    "    <li>Achieve an accuracy of more than 80 percent</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Custom Search\n",
    "<p>Since the original authors did not provide any database, the first thing we need to do is to scrape the images from the internet. For this we are going to use Google Custom Search and its JSON API.</p>\n",
    "<p>__Google Custom Search__ enables you to create a search engine for your website, your blog, or a collection of websites. You can configure your engine to search both web pages and images. You can fine-tune the ranking, add your own promotions and customize the look and feel of the search results. You can monetize the search by connecting your engine to your Google AdSense account[3].</p> \n",
    "<p>__Custom Search JSON API__ lets you develop websites and applications to retrieve and display search results from Google Custom Search programmatically. With this API, you can use RESTful requests to get either web search or image search results in JSON format[4].</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The API will return 10 image urls per query, and the free edition will only accept 100 queries per day. Also the number of images requested in one search cannot exceed 100, else the API will return a error. For more details on how to use the API, please visit go to the link in the citations[5]. The source code used to obtain the images used in this project can be found in the home directory under GetImage.py. Below are some examples of the obtained images.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Images/橋本奈々未/橋本奈々未.3.jpg\" alt=\"橋本奈々未.3.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Images/生田絵梨花/生田絵梨花.1.jpg\" alt=\"生田絵梨花.1.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Images/白石麻衣/白石麻衣.0.jpg\" alt=\"白石麻衣.0.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Images/西野七瀬/西野七瀬.5.jpg\" alt=\"西野七瀬.5.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Images/齋藤飛鳥/齋藤飛鳥.6.jpg\" alt=\"齋藤飛鳥.6.jpg\" style=\"width: 250px;\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pre-processing\n",
    "<p>Since we cannot and should not feed raw data directly into our network, the next step in the process is pre-processing the obtained images.\n",
    "</p>\n",
    "<ol>\n",
    "    <li>First we will use a face recognition algorithm to find the faces in each picture. Then we will crop and resize them to a format that the neural network can accept.</li>\n",
    "    <li>The second step will be to divide the data into training, and testing groups.</li>\n",
    "    <li>Because we are dealing with such a small amount of data, the third step will be data augmentation.</li>\n",
    "    <li>Finally, since we are going to use tensorflow and Keras, we will convert the images in to tensors(A form of data that tensorflow recognizes as input).</li>\n",
    "<ol/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoginize,  Crop, and Resize with OpenCV\n",
    "<p>For the recoginition, croping, and resizing process we will use the image processing library OpenCV. The OpenCV library comes with a method for face recognition called Haar Cascades. More details on Haar Cascades and its implemention can be found in the link in the citations[6]. Once the faces have been found they are cropped and resized to a dimention of 64 x 64. The code for this process is based on the code written by the original author[1] and can be found in the home directory under Preprocessing.py. Below are some examples of the cropped images.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Cropped/橋本奈々未/橋本奈々未.2.jpg\" alt=\"橋本奈々未.2.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Cropped/生田絵梨花/生田絵梨花.1.jpg\" alt=\"生田絵梨花.1.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Cropped/白石麻衣/白石麻衣.2.jpg\" alt=\"白石麻衣.2.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Cropped/西野七瀬/西野七瀬.5.jpg\" alt=\"西野七瀬.5.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Cropped/齋藤飛鳥/齋藤飛鳥.6.jpg\" alt=\"齋藤飛鳥.6.jpg\" style=\"width: 250px;\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the Dataset into Training and Test Subsets\n",
    "<p>We will be using the Deep Learning framework Keras with Tensorflow as backend for this project. It is common practice in deep learning to divide the data in to two subsets: training and testing. We will first create a root directory called \"Input_Data\". In this directory we will create the training and testing directories. And finally within each of those, we will have a total of five directories: one for each memeber. Of the total data, 70% was used for training and 30% was used for testing. The code for this process can be found in the home directory under Preprocessing.py.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After filtering out the defected images by hand, we only have 35~50 images for training and about 20 for testing. While we can train a model using the data at hand, due to the fact that dataset is simply too small, the model will fall into overfitting after a handful of epochs. Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. In order to fix this problem, we are going to use a method called data augmentation. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better[7].</p>\n",
    "<p>The images were augmented in 3 ways:\n",
    "<ol>\n",
    "    <li>Rotated at an angle of 10 to -10 degrees</li>\n",
    "    <li>A Threshold Filter was applied</li>\n",
    "    <li>A Gaussian Blur Filter with a kernel of (5, 5) was applied</li>\n",
    "</ol>\n",
    "</p>\n",
    "<p>\n",
    "This increases the amount of training data to 9 times its original size. Note that we are only augmenting the training data. The augmentation code was base on the following blog[2]. The testing data should not be alterned in any circumstances. The code for this process can be found in the home directory under Preprocessing.py. Below is an example of augmented pictures.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Input_Data/train/nanami/0_0.jpg\" alt=\"0_0.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Input_Data/train/nanami/0_-10.jpg\" alt=\"0_-10.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Input_Data/train/nanami/0_10.jpg\" alt=\"0_10.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Input_Data/train/nanami/0_0thr.jpg\" alt=\"0_0thr.jpg\" style=\"width: 250px;\"/></td>\n",
    "        <td><img src=\"Input_Data/train/nanami/0_0filter.jpg\" alt=\"0_0filter.jpg\" style=\"width: 250px;\"/></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Data into Tensors\n",
    "<p>Currently all the data are in jpeg format and the network does not support it as input. The data needs to be formatted into floating point tensors before being fed into our network. The required steps are as follows.\n",
    "<ol>\n",
    "    <li>Read the image files.\n",
    "    <li>Decode the JPEG content to RBG grids of pixels.\n",
    "    <li>Convert these into floating point tensors.\n",
    "    <li>Rescale the pixel values (between 0 and 255) to the [0, 1] interval (neural networks prefer to deal with small input values).\n",
    "</ol>\n",
    "<p>The code is base on the following blog[2] and can be found in the home directory under Basic_CNN.py</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Original CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, optimizers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "<p>This is the model used by the author of the original article. It consists of 4 sets of convolution and pooling layers and two dense layers. Usually, relu is used as the activation function for the dense layers, however according to the original artical, sigmoid was the better choice in this case (perhaps sigmoid is better for shallow networks). As can be seen in the summary below, this network has a input shape of (None, 64, 64, 3) and a output of (None, 5) with a total of 583,269 trainable parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (2, 2), input_shape=(64,64,3), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(32, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(32, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='sigmoid'))\n",
    "model.add(layers.Dense(128, activation='sigmoid'))\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         16512     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 583,269\n",
      "Trainable params: 583,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The model is compiled with:\n",
    "    <ul>\n",
    "        <li>__Loss Function__: Categorical Crossentropy</li>\n",
    "        <li>__Optimizer__: Stocastic Gradient Decent</li>\n",
    "        <li>__Metrics__: Accuracy</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>The model is trained with:\n",
    "    <ul>\n",
    "        <li>__Batch Size__: 32</li>\n",
    "        <li>__Epochs__: 100</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>The code for this section and the previous section can be found in the home directory under BasicCNN.py</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The two graphs above show the result of original network. After testing the network several times, I discovered that the maximum accuracy for the test data was 80(an average of 75). This is not an excellent result. In addition to that, the validation accuracy is very unstable.</p>\n",
    "<p>The most likely reason for this is the amount and quality of data. Since we only have 300~500 images per person(not to mention that most are augmented data) and 20 or less to validate the results on, we cannot expect good results without improvements to either the amount or the quality of data</p>\n",
    "<p>The second improvement that can be done is to the network. The original network is very simple and not optimized for face recognition. By using a model that is more suited to the task we can expect a rise in accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The three steps commonly involved in face recognition is as follows[3]:</p>\n",
    "<ol>\n",
    "    <li>Face Detection</li>\n",
    "    <li>Face Alignment</li>\n",
    "    <li>Face Recognition</li>\n",
    "</ol>\n",
    "<p>In the previous experiment with the original model, I skipped the second step. Therefore before moving on with optimizing the network we will begin with properly aligning the images we have. Here I will not go into depths about face alignment algorithms but rather utilize the model and algorithm implemented by dlib. The great thing about dlib is that it accomplishes the task of detection, alignment, and cropping all at once, reducing the amount of code we have to write. The original code can be found at dlib.net with the link in the citation[4]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "print('Using dlib version: {}'.format(dlib.__version__))\n",
    "print('Using cv2 version: {}'.format(cv2.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FaceAligner(face_file_path, output_path, predictor_path='./shape_predictor_5_face_landmarks.dat'):\n",
    "    # Load all the models we need: a detector to find the faces, a shape predictor\n",
    "    # to find face landmarks so we can precisely localize the face\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor(predictor_path)\n",
    "    \n",
    "    img=cv2.imread(face_file_path)\n",
    "    if img is None:\n",
    "        return\n",
    "    \n",
    "    b,g,r = cv2.split(img)\n",
    "    img = cv2.merge([r,g,b])\n",
    "    \n",
    "    # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
    "    # second argument indicates that we should upsample the image 1 time. This\n",
    "    # will make everything bigger and allow us to detect more faces.\n",
    "    dets = detector(img, 1)\n",
    "    num_faces = len(dets)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return\n",
    "    \n",
    "    # Find the 5 face landmarks we need to do the alignment.\n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(img, detection))\n",
    "\n",
    "    # Save Image\n",
    "    image = dlib.get_face_chip(img, faces[0])\n",
    "    dlib.save_image(image, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "FaceAligner('./images/橋本奈々未/橋本奈々未.3.jpg', './align_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = mpimg.imread('./images/橋本奈々未/橋本奈々未.3.jpg')\n",
    "aligned_img = mpimg.imread('./align_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_img)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Aligned Image')\n",
    "plt.imshow(aligned_img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"./images/*\" # The directory where the downloaded images are housed\n",
    "dst_dir=\"./aligned\" # The directory to place the cropped and resized images\n",
    "os.mkdir(dst_dir)\n",
    "src_dir=glob.glob(root)\n",
    "\n",
    "# Will crop and resize the downloaded images using OpenCV and place the results in the destination directory declared above\n",
    "for path in src_dir:\n",
    "    dst = os.path.join(dst_dir, path.split('/')[2])\n",
    "    os.mkdir(dst)\n",
    "    for img in os.listdir(path):\n",
    "        FaceAligner(os.path.join(path, img), os.path.join(dst, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory that will hold the data\n",
    "base_dir='./input_data2'\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directories to house the training images, validation images, and test images\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the directory to house the training images\n",
    "train_erika_dir = os.path.join(train_dir, 'erika')\n",
    "os.mkdir(train_erika_dir)\n",
    "train_asuka_dir = os.path.join(train_dir, 'asuka')\n",
    "os.mkdir(train_asuka_dir)\n",
    "train_mai_dir = os.path.join(train_dir, 'mai')\n",
    "os.mkdir(train_mai_dir)\n",
    "train_nanase_dir = os.path.join(train_dir, 'nanase')\n",
    "os.mkdir(train_nanase_dir)\n",
    "train_nanami_dir = os.path.join(train_dir, 'nanami')\n",
    "os.mkdir(train_nanami_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to house the testing images\n",
    "test_erika_dir = os.path.join(test_dir, 'erika')\n",
    "os.mkdir(test_erika_dir)\n",
    "test_asuka_dir = os.path.join(test_dir, 'asuka')\n",
    "os.mkdir(test_asuka_dir)\n",
    "test_mai_dir = os.path.join(test_dir, 'mai')\n",
    "os.mkdir(test_mai_dir)\n",
    "test_nanase_dir = os.path.join(test_dir, 'nanase')\n",
    "os.mkdir(test_nanase_dir)\n",
    "test_nanami_dir = os.path.join(test_dir, 'nanami')\n",
    "os.mkdir(test_nanami_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/生田絵梨花/*\") \n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/齋藤飛鳥/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/白石麻衣/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/橋本奈々未/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/西野七瀬/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"asuka\",\"mai\",\"erika\",\"nanami\",\"nanase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the training data\n",
    "train_dir=\"./input_data2/train/\"\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(train_dir+name[i])\n",
    "    print('Found {} training images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(train_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img = cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # refactoring the image\n",
    "        img = np.divide(img, 255)\n",
    "        X_train.append(img)\n",
    "        Y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the validation data\n",
    "validation_dir=\"./input_data2/test/\"\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(validation_dir+name[i])\n",
    "    print('Found {} testing images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(validation_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img=cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # Refactoring the images\n",
    "        img = np.divide(img, 255)\n",
    "        X_test.append(img)\n",
    "        Y_test.append(i)\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, I created a new network. The difference between the previous network is that there is one more set of convolution/pooling layers, and that the feature maps are deeper. Also the optimizer was changed to RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the orignal network, this one learns faster, achieving a accuracy rate of roughly 75 percent after only 25 epochs. At the same time it falls into overfitting much faster while the accuracy peaks at about the same as the previous network. In order to reduce overfitting I placed a dropout layer (with a dropout rate of 0.5) into the network, but the results instead of improving, the results were catastrophic. I adjusted the dropout rate and also tried a few other regularization techniques such as L2 and L3 regularizations but all these methods do is reduce the rate of overfitting. This  it would prove very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pretrained Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A common and highly effective approach to deep learning on small image datasets is to leverage a pretrained network. A pretrained network is simply a saved network previously trained on a large dataset, typically on a large-scale image classification task. If the original dataset is large enough and general enough, the features of the network can act as an effective model of the visual world, therefore its feature can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from the original task. For instance, one might train a network on ImageNet (where classes are mostly animals and everyday objects) and then reuse this network for identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning comapared to many older shallow learning approaches, and it makes deep learning very effective for small-data problems.</p>\n",
    "\n",
    "<p>For this problem I will use a large convnet trained on the ImageNet dataset(1.4 million labeled images and 1000 different classes). I must note however that ImageNet is a dataset for object recognition not face recognition, thus it is not ideal. However the Keras provides easy access to networks trained on the dataset and it is more preferable to training a vast network from scratch myself.</p>\n",
    "\n",
    "<p>For my first attempt I will use a architecture called VGG16, developed by Karen Simonyan and Andrew Zisserman in 2014. The network is simple, widely used, and simple to understand. It is a bit of an older model, and cannot compare with the current state of the art heavier ones, but I prefer it to the other because I can understand the underlying concepts.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. I will start with feature extraction. Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.</p>\n",
    "\n",
    "<p> As can be seen in the earlier examples, convnets used for image classification are comprised from two parts: a series of pooling and convolution layers and a densely-connected classifier. The first part is called the \"convolutional base\". In the case of feature extraction, we simply take the convolutional base of a previously-trained network, run the new data through it, and train a new classifier on top of the output.</p>\n",
    "\n",
    "<p>The reason we only reuse the convolution base is, that the representations learned by the convolution base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer vision problem at hand. On the other hand, the representations learned by the classifier will only contain information specific to the set of classes that the model was trained on. Additionally, representations found in densely-connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely-connected features would be largely useless.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGGFace(include_top=False, input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The output above shows a summary of the VGG16 model. The classification layers has been removed from the model upon import therefore we are only looking at the convolution base. As you can see, the model is very simple and can be understood very easily. It is comprised of 5 blocks of convolution and pooling layers and has a total of 14,714,688 parameters, which is very large. The classifier I will be adding on top has 2 million parameters.</p>\n",
    "\n",
    "<p>However before we can complie and train our model, a very important thing to do is to freeze the convolutional base. \"Freezing\" a layer of set of layers means preventing their weights from getting updated during training. If we don't do this, then the representations that were previously learned by the convolutional base would get modified during training. Since the Dense layers on top are randomly intialized, very large weight updates would be propagated throught the network, effectively destroying the representations previously learned.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We must note here that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local. highly generic feature maps (such as visual edges, colors, and textures), while layers higher-up extract more abstract concepts. So if the new dataset differs a lot from the dataset that the original model was trained on, it might be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.</p>\n",
    "\n",
    "<p>In our case, the ImageNet class set did contain people, however the dataset is not optimized for face recognition, thus it is best to remove the classifier and use only the top layers of the network. That is where the second method, fine tuning, comes in.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Fine tuning is another technique for model reuse that is complementary to feature extraction. While feature extraction reuses the weight of the pre-trained model by freezing layers, fine tuning does the opposite and unfreezes the top layer of the model. It is called \"fine-tuning\" because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.</p>\n",
    "\n",
    "<p>As I explained previously, it is necessary to freeze the convolution base in order to be able to train a randomly intialized classifier on top. For the smae reason, it is only possible to fine-tune the top layer of the convolution base once the classifier on top has already been trained. Thus the steps for fine-tuning a network are as follow:\n",
    "<ol>\n",
    "    <li>Add your custom network on top of an already trained base network.\n",
    "    <li>Freeze the base network.\n",
    "    <li>Train the part you added.\n",
    "    <li>Unfreeze some layers in the base network.\n",
    "    <li>Jointly train both these layers and the part you added. \n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom parameters\n",
    "nb_class = 5\n",
    "hidden_dim = 512\n",
    "\n",
    "last_layer = conv_base.get_layer('pool5').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
    "out = Dense(5, activation='softmax', name='fc8')(x)\n",
    "model = Model(conv_base.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the Network\n",
    "print('This is the number of trainable weights '\n",
    "      'before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights '\n",
    "      'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the batch generator to fit the model to the data\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=29,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot the loss value\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "<p>\n",
    "    <ol>\n",
    "        <li>“機械学習で乃木坂46を顏分類してみた.” Aidemy Blog, 株式会社アイデミー, 4 Dec. 2018, blog.aidemy.net/entry/2017/12/17/214715.</li>\n",
    "        <li>nirs_kd56. “乃木坂メンバーの顔をCNNで分類.” Qiita, 26 Sept. 2018, qiita.com/nirs_kd56/items/bc78bf2c3164a6da1ded.</li>\n",
    "        <li>Google Customs Search Website. developers.google.com/custom-search/</li>\n",
    "        <li>Google Custom Search JSON API. developers.google.com/custom-search/v1/overview</li>\n",
    "        <li>@onlyzs. \"Google Custom Search APIを使って画像収集\". Qiita, 24 Nov. 2016, qiita.com/onlyzs/items/c56fb76ce43e45c12339</li>\n",
    "        <li>“Face Detection Using Haar Cascades.” Cascade Classification - OpenCV 2.4.13.7 Documentation, docs.opencv.org/3.4/d7/d8b/tutorial_py_face_detection.html.</li>\n",
    "        <li>Fchollet. “Fchollet/Deep-Learning-with-Python-Notebooks.” GitHub, github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.2-using-convnets-with-small-datasets.ipynb.</li>\n",
    "        <li>Lian, Qianli. A Summary of Deep Models for Face Recognition. cs.wellesley.edu/~vision/slides/Qianli_summary_deep_face_models.pdf.</li>\n",
    "        <li>“Face Alignment - Dlib.” Dlib C++ Library, dlib.net/face_alignment.py.html.</li>\n",
    "        <li>Raghuvanshi, Arushi, and Vivek Choksi. Facial Expression Recognition with Convolutional Neural Networks. cs231n.stanford.edu/reports/2016/pdfs/023_Report.pdf.</li>\n",
    "        <li>A. V. Omkar M. Parkhi and A. Zisserman. Deep face recog-\n",
    "nition. 2015. www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf.</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
