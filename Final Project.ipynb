{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the faces of Nogizaka 46 with CNN\n",
    "<p>Student ID: 71745242<br>\n",
    "Name: Shozen Dan <br>\n",
    "Class: Heuristic Computing <br>\n",
    "Instructor: Takefuji Yoshiyasu</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective\n",
    "<p>The other day, when I was surfing the internet for interesting machine learning ideas, I discovered a blog on Aidemy about facial identification[1]. The author is a fan of the popular idol group Nogizaka 46 and wanted to create a program that would identify the faces of his favorite 5 memebers. The steps he took are as simple. He obtained 100 images for each of the target members using Google's custom search API. Next he used the image processing library OpenCV to find the faces in each picture and crop them out. At this point there were only about 70 images remaining for each member, thus he augmented the data and increased the data size 8 times. Finally, he created a network using Keras and Tesorflow and achieved an accuracy of about 70 percent. I found another blog[2] trying to solve the same problem and the author achieved an accuracy of 75~80 percent. Although the problem its self is rather frivolous and have no practical applications, the steps involved in solving it can be applied widely. Thus the objective of my project is as follows</p>\n",
    "<ol>\n",
    "    <li>Learn the methods involved in dealing with small datasets for deep learning</li>\n",
    "    <li>Achieve an accuracy of more than 80 percent</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtaining Data\n",
    "<p>The first step is to obtain the images. Since there were no database, I need to scrape the images from the internet. For this I am going to use Google's Custom Search API. It enables us to obtain the search results for specific key words. For more on how to use the API, please visit the link in the citation[2].</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Custom Search API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "import json\n",
    "import os \n",
    "import urllib.request\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import shutil\n",
    "from scipy import ndimage\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up basic configuration parameters. The API will return links to 10 images per query, and the free edition only accepts 100 queries per day. Also the number of images requested in one search cannot exceed 100, else the API will return a error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY='AIzaSyDje-1_ZTr3amF89Yy3fI6aSnTqp1xgufc'\n",
    "CUSTOM_SEARCH_ENGINE='018321335581135235938:lyu926pg63g'\n",
    "KEYWORDS=[\"生田絵梨花\",\"齋藤飛鳥\",\"白石麻衣\",\"西野七瀬\",\"橋本奈々未\"]\n",
    "NUM_OF_IMAGES=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This function takes the list of seach keywords and the number of images as parameters. It will use Google's custom search API to obtain the links to each image and return them a list containing the links.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageUrl(search_item: list, total_num: int):\n",
    "    img_list = []\n",
    "    i = 0\n",
    "    while i < total_num:\n",
    "        query_img = \"https://www.googleapis.com/customsearch/v1?key=\" + API_KEY + \"&cx=\" + CUSTOM_SEARCH_ENGINE + \"&num=\" + str(10 if(total_num-i)>10 else (total_num-i)) + \"&start=\" + str(i+1) + \"&q=\" + quote(search_item) + \"&searchType=image\"\n",
    "        res = urllib.request.urlopen(query_img)\n",
    "        data = json.loads(res.read().decode('utf-8'))\n",
    "        for j in range(len(data['items'])):\n",
    "            img_list.append(data['items'][j]['link'])\n",
    "        i=i+10\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the list of search keywords, the list of links to images, and the path of the directory to house the downloaded images. It will create a directory for each keyword and place the downloaded images in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(search_item: list, img_list: list, base_dir_name='images'):\n",
    "    os.mkdir(base_dir_name) # create base dir\n",
    "    item_dir = os.path.join(base_dir_name, search_item)\n",
    "    os.mkdir(item_dir) # directory to house the images\n",
    "    http = httplib2.Http(\".cache\") # Initiate http request object instance \n",
    "    for i in range(len(img_list)):\n",
    "        try:\n",
    "            response, content = http.request(img_list[i])\n",
    "            filename = os.path.join(item_dir, search_item + '.' + str(i) + '.jpg')\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(content)\n",
    "        except:\n",
    "            print('Error: failed to download image')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(KEYWORDS)):\n",
    "    print('=== downloading images for {} ==='.format(KEYWORDS[j]))\n",
    "    img_list=getImageUrl(KEYWORDS[j],NUM_OF_IMAGES)\n",
    "    getImage(KEYWORDS[j], img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "<p>The next step in the process is pre-processing our data. The first step is using a face recognition algorithm to find faces in each picture, crop and resize them to a format that the neural network can accept as input. The second step will be to divide the data into train, validation, and test groups. The third step is to convert in the images in to tensors(A form of data that the tensorflow network recognizes as input). The final step is augmenting the data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoginize,  Crop, and Resize with OpenCV\n",
    "<p>Here I will use the image processing library OpenCV to recognize, crop, and resize the faces in each image. I usedf a method called haarcascades. More on haarcascades can be found in the link in the citations[3].</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"./images/*\" # The directory where the downloaded images are housed\n",
    "dst_dir=\"./cropped\" # The directory to place the cropped and resized images\n",
    "os.mkdir(dst_dir)\n",
    "src_dir=glob.glob(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will crop and resize the downloaded images using OpenCV and place the results in the destination directory declared above\n",
    "for path in src_dir:\n",
    "    dst = os.path.join(dst_dir, path.split('/')[2])\n",
    "    os.mkdir(dst)\n",
    "    for img in os.listdir(path):\n",
    "        image = cv2.imread(os.path.join(path, img))\n",
    "        if image is None:\n",
    "            continue\n",
    "        image_grey=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert image to greyscale\n",
    "        cascade=cv2.CascadeClassifier(\"/usr/local/opt/opencv/share/OpenCV/haarcascades/haarcascade_frontalface_alt.xml\") #Import Classifier\n",
    "        face_list=face_list=cascade.detectMultiScale(image_grey, scaleFactor=1.1, minNeighbors=2,minSize=(64,64)) # Face recognition\n",
    "        # If faces were detected\n",
    "        if len(face_list) > 0: \n",
    "            for rect in face_list:\n",
    "                x,y,width,height=rect\n",
    "                image=image[rect[1]:rect[1]+rect[3],rect[0]:rect[0]+rect[2]]\n",
    "                if image.shape[0] < 64: \n",
    "                    continue\n",
    "                image = cv2.resize(image,(64,64))\n",
    "            fileName=os.path.join(dst, img)\n",
    "            cv2.imwrite(fileName, image) # Save image\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the Dataset into Training, Validation, and Test Subsets\n",
    "<p>I will be using the Deep-learning framework Keras for this project. It is common practice in deep learning to divide the data in to three subsets: training, validation, and testing. The code below is mostly about creating the directories and moving the cropped pictures into them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory that will hold the data\n",
    "base_dir='./input_data'\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directories to house the training images, validation images, and test images\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the directory to house the training images\n",
    "train_erika_dir = os.path.join(train_dir, 'erika')\n",
    "os.mkdir(train_erika_dir)\n",
    "train_asuka_dir = os.path.join(train_dir, 'asuka')\n",
    "os.mkdir(train_asuka_dir)\n",
    "train_mai_dir = os.path.join(train_dir, 'mai')\n",
    "os.mkdir(train_mai_dir)\n",
    "train_nanase_dir = os.path.join(train_dir, 'nanase')\n",
    "os.mkdir(train_nanase_dir)\n",
    "train_nanami_dir = os.path.join(train_dir, 'nanami')\n",
    "os.mkdir(train_nanami_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to house the testing images\n",
    "test_erika_dir = os.path.join(test_dir, 'erika')\n",
    "os.mkdir(test_erika_dir)\n",
    "test_asuka_dir = os.path.join(test_dir, 'asuka')\n",
    "os.mkdir(test_asuka_dir)\n",
    "test_mai_dir = os.path.join(test_dir, 'mai')\n",
    "os.mkdir(test_mai_dir)\n",
    "test_nanase_dir = os.path.join(test_dir, 'nanase')\n",
    "os.mkdir(test_nanase_dir)\n",
    "test_nanami_dir = os.path.join(test_dir, 'nanami')\n",
    "os.mkdir(test_nanami_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippets of code, moves the cropped images in to the directories created above. Note that 70 percent of the images are used for training, 20 for validation, and 10 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./cropped/生田絵梨花/*\") \n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./cropped/齋藤飛鳥/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./cropped/白石麻衣/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./cropped/橋本奈々未/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./cropped/西野七瀬/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After filtering out the defected images by hand, we only have 35~50 images for training and about 20 for testing. While I can train a model using the data at hand, due to the fact that dataset is simply too small, the model will fall into overfitting after a handful of epochs. Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. In order to fix this problem, I am going to use a method called data augmentation. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"asuka\",\"mai\",\"erika\",\"nanami\",\"nanase\"]\n",
    "for name in names:\n",
    "    in_dir = \"./input_data/train/\"+name+\"/*\"\n",
    "    out_dir = \"./input_data/train/\"+name\n",
    "    in_jpg=glob.glob(in_dir)\n",
    "    img_file_name_list=os.listdir(\"./input_data/train/\"+name+\"/\")\n",
    "    for i in range(len(in_jpg)):\n",
    "        img = cv2.imread(str(in_jpg[i]))\n",
    "        # Rotate Images\n",
    "        for ang in [-10,0,10]:\n",
    "            img_rot = ndimage.rotate(img,ang)\n",
    "            img_rot = cv2.resize(img_rot,(64,64))\n",
    "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\".jpg\")\n",
    "            cv2.imwrite(str(fileName),img_rot)\n",
    "            # Threshold\n",
    "            img_thr = cv2.threshold(img_rot, 100, 255, cv2.THRESH_TOZERO)[1]\n",
    "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\"thr.jpg\")\n",
    "            cv2.imwrite(str(fileName),img_thr)\n",
    "            # Filter Images\n",
    "            img_filter = cv2.GaussianBlur(img_rot, (5, 5), 0)\n",
    "            fileName=os.path.join(out_dir,str(i)+\"_\"+str(ang)+\"filter.jpg\")\n",
    "            cv2.imwrite(str(fileName),img_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training erika images: ', len(os.listdir(train_erika_dir)))\n",
    "print('total test erika images: ', len(os.listdir(test_erika_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training asuka images: ', len(os.listdir(train_asuka_dir)))\n",
    "print('total test asuka images: ', len(os.listdir(test_asuka_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training mai images: ', len(os.listdir(train_mai_dir)))\n",
    "print('total test mai images: ', len(os.listdir(test_mai_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training nanase images: ', len(os.listdir(train_nanase_dir)))\n",
    "print('total test nanase images: ', len(os.listdir(test_nanase_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training nanami images: ', len(os.listdir(train_nanami_dir)))\n",
    "print('total test nanami images: ', len(os.listdir(test_nanami_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Data into Tensors\n",
    "<p>Currently the images are in jpeg format and the network doest not support it as input. Data need to be formatted into appropriately pre-processed floating point tensors before being fed into our network. The required steps are as follows.\n",
    "<ol>\n",
    "    <li>Read the picture files.\n",
    "    <li>Decode the JPEG content to RBG grids of pixels.\n",
    "    <li>Convert these into floating point tensors.\n",
    "    <li>Rescale the pixel values (between 0 and 255) to the [0, 1] interval (neural networks prefer to deal with small input values).\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"asuka\",\"mai\",\"erika\",\"nanami\",\"nanase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the training data\n",
    "train_dir=\"./input_data/train/\"\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(train_dir+name[i])\n",
    "    print('Found {} training images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(train_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img = cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # refactoring the image\n",
    "        img = np.divide(img, 255)\n",
    "        X_train.append(img)\n",
    "        Y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the validation data\n",
    "validation_dir=\"./input_data/test/\"\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(validation_dir+name[i])\n",
    "    print('Found {} testing images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(validation_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img=cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # Refactoring the images\n",
    "        img = np.divide(img, 255)\n",
    "        X_test.append(img)\n",
    "        Y_test.append(i)\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model used by the author of the original article. It consists of 4 sets of convolution/pooling layers and two dense layerz. Usually, relu is used as the activation function, however according to the article, sigmoid is the better choice in this case(perhaps sigmoid is better for shallow networks). The loss function used to evaluate the network is categorical crossentropy, and the optimizer is standard gradient decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Input\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32, (2, 2), input_shape=(64,64,3), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(32, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(32, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (2, 2), strides=(1,1), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='sigmoid'))\n",
    "model.add(layers.Dense(128, activation='sigmoid'))\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the batch generator to fit the model to the data\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=32,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ngz46_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('./orig_acc.png')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot the loss value\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.savefig('./orig_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The two graphs above show the result of original network. After testing the network several times, I discovered that the maximum accuracy for the test data was 80(an average of 75). This is not an excellent result. In addition to that, the validation accuracy is very unstable.</p>\n",
    "<p>The most likely reason for this is the amount and quality of data. Since we only have 300~500 images per person(not to mention that most are augmented data) and 20 or less to validate the results on, we cannot expect good results without improvements to either the amount or the quality of data</p>\n",
    "<p>The second improvement that can be done is to the network. The original network is very simple and not optimized for face recognition. By using a model that is more suited to the task we can expect a rise in accuracy.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The three steps commonly involved in face recognition is as follows[3]:</p>\n",
    "<ol>\n",
    "    <li>Face Detection</li>\n",
    "    <li>Face Alignment</li>\n",
    "    <li>Face Recognition</li>\n",
    "</ol>\n",
    "<p>In the previous experiment with the original model, I skipped the second step. Therefore before moving on with optimizing the network we will begin with properly aligning the images we have. Here I will not go into depths about face alignment algorithms but rather utilize the model and algorithm implemented by dlib. The great thing about dlib is that it accomplishes the task of detection, alignment, and cropping all at once, reducing the amount of code we have to write. The original code can be found at dlib.net with the link in the citation[4]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dlib version: 19.16.0\n",
      "Using cv2 version: 3.4.1\n"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "print('Using dlib version: {}'.format(dlib.__version__))\n",
    "print('Using cv2 version: {}'.format(cv2.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FaceAligner(face_file_path, output_path, predictor_path='./shape_predictor_5_face_landmarks.dat'):\n",
    "    # Load all the models we need: a detector to find the faces, a shape predictor\n",
    "    # to find face landmarks so we can precisely localize the face\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    sp = dlib.shape_predictor(predictor_path)\n",
    "    \n",
    "    img=cv2.imread(face_file_path)\n",
    "    if img is None:\n",
    "        return\n",
    "    \n",
    "    b,g,r = cv2.split(img)\n",
    "    img = cv2.merge([r,g,b])\n",
    "    \n",
    "    # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
    "    # second argument indicates that we should upsample the image 1 time. This\n",
    "    # will make everything bigger and allow us to detect more faces.\n",
    "    dets = detector(img, 1)\n",
    "    num_faces = len(dets)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return\n",
    "    \n",
    "    # Find the 5 face landmarks we need to do the alignment.\n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        faces.append(sp(img, detection))\n",
    "\n",
    "    # Save Image\n",
    "    image = dlib.get_face_chip(img, faces[0])\n",
    "    dlib.save_image(image, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "FaceAligner('./images/橋本奈々未/橋本奈々未.3.jpg', './align_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = mpimg.imread('./images/橋本奈々未/橋本奈々未.3.jpg')\n",
    "aligned_img = mpimg.imread('./align_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Original Image')\n",
    "plt.imshow(original_img)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Aligned Image')\n",
    "plt.imshow(aligned_img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"./images/*\" # The directory where the downloaded images are housed\n",
    "dst_dir=\"./aligned\" # The directory to place the cropped and resized images\n",
    "os.mkdir(dst_dir)\n",
    "src_dir=glob.glob(root)\n",
    "\n",
    "# Will crop and resize the downloaded images using OpenCV and place the results in the destination directory declared above\n",
    "for path in src_dir:\n",
    "    dst = os.path.join(dst_dir, path.split('/')[2])\n",
    "    os.mkdir(dst)\n",
    "    for img in os.listdir(path):\n",
    "        FaceAligner(os.path.join(path, img), os.path.join(dst, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory that will hold the data\n",
    "base_dir='./input_data2'\n",
    "os.mkdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directories to house the training images, validation images, and test images\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the directory to house the training images\n",
    "train_erika_dir = os.path.join(train_dir, 'erika')\n",
    "os.mkdir(train_erika_dir)\n",
    "train_asuka_dir = os.path.join(train_dir, 'asuka')\n",
    "os.mkdir(train_asuka_dir)\n",
    "train_mai_dir = os.path.join(train_dir, 'mai')\n",
    "os.mkdir(train_mai_dir)\n",
    "train_nanase_dir = os.path.join(train_dir, 'nanase')\n",
    "os.mkdir(train_nanase_dir)\n",
    "train_nanami_dir = os.path.join(train_dir, 'nanami')\n",
    "os.mkdir(train_nanami_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory to house the testing images\n",
    "test_erika_dir = os.path.join(test_dir, 'erika')\n",
    "os.mkdir(test_erika_dir)\n",
    "test_asuka_dir = os.path.join(test_dir, 'asuka')\n",
    "os.mkdir(test_asuka_dir)\n",
    "test_mai_dir = os.path.join(test_dir, 'mai')\n",
    "os.mkdir(test_mai_dir)\n",
    "test_nanase_dir = os.path.join(test_dir, 'nanase')\n",
    "os.mkdir(test_nanase_dir)\n",
    "test_nanami_dir = os.path.join(test_dir, 'nanami')\n",
    "os.mkdir(test_nanami_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/生田絵梨花/*\") \n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_erika_dir, 'erika.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/齋藤飛鳥/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_asuka_dir, 'asuka.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/白石麻衣/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_mai_dir, 'mai.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/橋本奈々未/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanase_dir, 'nanami.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob.glob(\"./aligned/西野七瀬/*\")\n",
    "train_len = math.floor(len(fnames) * 0.7)\n",
    "for i in range(len(fnames)):\n",
    "    if i < train_len:\n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(train_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)\n",
    "    else: \n",
    "        src = fnames[i]\n",
    "        dst = os.path.join(test_nanami_dir, 'nanase.{}.jpg'.format(i))\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"asuka\",\"mai\",\"erika\",\"nanami\",\"nanase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the training data\n",
    "train_dir=\"./input_data2/train/\"\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(train_dir+name[i])\n",
    "    print('Found {} training images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(train_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img = cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # refactoring the image\n",
    "        img = np.divide(img, 255)\n",
    "        X_train.append(img)\n",
    "        Y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the validation data\n",
    "validation_dir=\"./input_data2/test/\"\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(len(name)):\n",
    "    img_file_name_list=os.listdir(validation_dir+name[i])\n",
    "    print('Found {} testing images for {}'.format(len(img_file_name_list), name[i]))\n",
    "    for j in range(0, len(img_file_name_list)-1):\n",
    "        n=os.path.join(validation_dir+name[i]+\"/\", img_file_name_list[j])\n",
    "        img=cv2.imread(n)\n",
    "        b,g,r = cv2.split(img)\n",
    "        img = cv2.merge([r,g,b])\n",
    "        # Refactoring the images\n",
    "        img = np.divide(img, 255)\n",
    "        X_test.append(img)\n",
    "        Y_test.append(i)\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, I created a new network. The difference between the previous network is that there is one more set of convolution/pooling layers, and that the feature maps are deeper. Also the optimizer was changed to RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the orignal network, this one learns faster, achieving a accuracy rate of roughly 75 percent after only 25 epochs. At the same time it falls into overfitting much faster while the accuracy peaks at about the same as the previous network. In order to reduce overfitting I placed a dropout layer (with a dropout rate of 0.5) into the network, but the results instead of improving, the results were catastrophic. I adjusted the dropout rate and also tried a few other regularization techniques such as L2 and L3 regularizations but all these methods do is reduce the rate of overfitting. This  it would prove very difficult to go any higher just by training our own convnet from scratch, simply because we have so little data to work with. As a next step to improve our accuracy on this problem, we will have to leverage a pre-trained model, which will be the focus of the next two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pretrained Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A common and highly effective approach to deep learning on small image datasets is to leverage a pretrained network. A pretrained network is simply a saved network previously trained on a large dataset, typically on a large-scale image classification task. If the original dataset is large enough and general enough, the features of the network can act as an effective model of the visual world, therefore its feature can prove useful for many different computer vision problems, even though these new problems might involve completely different classes from the original task. For instance, one might train a network on ImageNet (where classes are mostly animals and everyday objects) and then reuse this network for identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning comapared to many older shallow learning approaches, and it makes deep learning very effective for small-data problems.</p>\n",
    "\n",
    "<p>For this problem I will use a large convnet trained on the ImageNet dataset(1.4 million labeled images and 1000 different classes). I must note however that ImageNet is a dataset for object recognition not face recognition, thus it is not ideal. However the Keras provides easy access to networks trained on the dataset and it is more preferable to training a vast network from scratch myself.</p>\n",
    "\n",
    "<p>For my first attempt I will use a architecture called VGG16, developed by Karen Simonyan and Andrew Zisserman in 2014. The network is simple, widely used, and simple to understand. It is a bit of an older model, and cannot compare with the current state of the art heavier ones, but I prefer it to the other because I can understand the underlying concepts.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There are two ways to leverage a pre-trained network: feature extraction and fine-tuning. I will start with feature extraction. Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch.</p>\n",
    "\n",
    "<p> As can be seen in the earlier examples, convnets used for image classification are comprised from two parts: a series of pooling and convolution layers and a densely-connected classifier. The first part is called the \"convolutional base\". In the case of feature extraction, we simply take the convolutional base of a previously-trained network, run the new data through it, and train a new classifier on top of the output.</p>\n",
    "\n",
    "<p>The reason we only reuse the convolution base is, that the representations learned by the convolution base are likely to be more generic and therefore more reusable: the feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer vision problem at hand. On the other hand, the representations learned by the classifier will only contain information specific to the set of classes that the model was trained on. Additionally, representations found in densely-connected layers no longer contain any information about where objects are located in the input image: these layers get rid of the notion of space, whereas the object location is still described by convolutional feature maps. For problems where object location matters, densely-connected features would be largely useless.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine import  Model\n",
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras_vggface.vggface import VGGFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGGFace(include_top=False, input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_1 (Conv2D)             (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv1_2 (Conv2D)             (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2_1 (Conv2D)             (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2_2 (Conv2D)             (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv3_1 (Conv2D)             (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv3_2 (Conv2D)             (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv3_3 (Conv2D)             (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv4_1 (Conv2D)             (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv4_2 (Conv2D)             (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv4_3 (Conv2D)             (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv5_1 (Conv2D)             (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv5_2 (Conv2D)             (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv5_3 (Conv2D)             (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The output above shows a summary of the VGG16 model. The classification layers has been removed from the model upon import therefore we are only looking at the convolution base. As you can see, the model is very simple and can be understood very easily. It is comprised of 5 blocks of convolution and pooling layers and has a total of 14,714,688 parameters, which is very large. The classifier I will be adding on top has 2 million parameters.</p>\n",
    "\n",
    "<p>However before we can complie and train our model, a very important thing to do is to freeze the convolutional base. \"Freezing\" a layer of set of layers means preventing their weights from getting updated during training. If we don't do this, then the representations that were previously learned by the convolutional base would get modified during training. Since the Dense layers on top are randomly intialized, very large weight updates would be propagated throught the network, effectively destroying the representations previously learned.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We must note here that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local. highly generic feature maps (such as visual edges, colors, and textures), while layers higher-up extract more abstract concepts. So if the new dataset differs a lot from the dataset that the original model was trained on, it might be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base.</p>\n",
    "\n",
    "<p>In our case, the ImageNet class set did contain people, however the dataset is not optimized for face recognition, thus it is best to remove the classifier and use only the top layers of the network. That is where the second method, fine tuning, comes in.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Fine tuning is another technique for model reuse that is complementary to feature extraction. While feature extraction reuses the weight of the pre-trained model by freezing layers, fine tuning does the opposite and unfreezes the top layer of the model. It is called \"fine-tuning\" because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.</p>\n",
    "\n",
    "<p>As I explained previously, it is necessary to freeze the convolution base in order to be able to train a randomly intialized classifier on top. For the smae reason, it is only possible to fine-tune the top layer of the convolution base once the classifier on top has already been trained. Thus the steps for fine-tuning a network are as follow:\n",
    "<ol>\n",
    "    <li>Add your custom network on top of an already trained base network.\n",
    "    <li>Freeze the base network.\n",
    "    <li>Train the part you added.\n",
    "    <li>Unfreeze some layers in the base network.\n",
    "    <li>Jointly train both these layers and the part you added. \n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom parameters\n",
    "nb_class = 5\n",
    "hidden_dim = 512\n",
    "\n",
    "last_layer = conv_base.get_layer('pool5').output\n",
    "x = Flatten(name='flatten')(last_layer)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
    "x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
    "out = Dense(5, activation='softmax', name='fc8')(x)\n",
    "model = Model(conv_base.input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing the Network\n",
    "print('This is the number of trainable weights '\n",
    "      'before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights '\n",
    "      'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='sgd',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the batch generator to fit the model to the data\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=29,\n",
    "                    epochs=25,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot the loss value\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "<p>\n",
    "    <ol>\n",
    "        <li>“機械学習で乃木坂46を顏分類してみた.” Aidemy Blog, 株式会社アイデミー, 4 Dec. 2018, blog.aidemy.net/entry/2017/12/17/214715.</li>\n",
    "        <li>nirs_kd56. “乃木坂メンバーの顔をCNNで分類.” Qiita, 26 Sept. 2018, qiita.com/nirs_kd56/items/bc78bf2c3164a6da1ded.</li>\n",
    "        <li>Lian, Qianli. A Summary of Deep Models for Face Recognition. cs.wellesley.edu/~vision/slides/Qianli_summary_deep_face_models.pdf.</li>\n",
    "        <li>“Face Alignment - Dlib.” Dlib C++ Library, dlib.net/face_alignment.py.html.</li>\n",
    "        <li>Raghuvanshi, Arushi, and Vivek Choksi. Facial Expression Recognition with Convolutional Neural Networks. cs231n.stanford.edu/reports/2016/pdfs/023_Report.pdf.</li>\n",
    "        <li>A. V. Omkar M. Parkhi and A. Zisserman. Deep face recog-\n",
    "nition. 2015. www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf.</li>\n",
    "    </ol>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
